{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dev\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoWModel(object):\n",
    "    \n",
    "    def __init__(self, train_fname, embedding_fname,\n",
    "                model_fname, embedding_corpus_fname,\n",
    "                embedding_method='fasttext', is_weighted=True,\n",
    "                average=False, dim=100, tokenizer_name='mecab'):\n",
    "        # configurations\n",
    "        make_save_path(model_fname)\n",
    "        self.dim = dim\n",
    "        self.average = average\n",
    "        if is_weighted:\n",
    "            model_full_fname = model_fname + '-weighted'\n",
    "        else:\n",
    "            model_full_fname = model_fname + '-original'\n",
    "        self.tokenizer = get_tokenizer(tokenizer_name)\n",
    "        if is_weighted:\n",
    "            # weighted embeddings\n",
    "            self.embeddings = \\\n",
    "                self.load_or_construct_weighted_embedding(embedding_fname, \n",
    "                                                         embedding_method, embedding_corpus_fname)\n",
    "            print('loading weighted embeddings, complete!')\n",
    "        else:\n",
    "            # original embeddings\n",
    "            words, vectors = self.load_word_embeddings(embedding_fname,embedding_method)\n",
    "            self.embeddings = defaultdict(list)\n",
    "            for word, vector in zip(words, vectors):\n",
    "                self.embeddings[word] = vector\n",
    "            print('loading original embeddings, complete!')\n",
    "        if not os.path.exists(model_full_name):\n",
    "            print('trian Continuous Bag of Words model')\n",
    "            self.model = self.train_model(train_fname, model_full_name)\n",
    "        else:\n",
    "            print('load Continuous Bag of Words model')\n",
    "            self.model = self.load_model(model_full_fname)\n",
    "            \n",
    "    def compute_word_frequency(self, embedding_corpus_fname):\n",
    "        total_count = 0\n",
    "        words_count = defaultdict(int)\n",
    "        with open(embedding_corpus_fname, 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                for token in tokens:\n",
    "                    words_count[token] += 1\n",
    "                    total_count += 1\n",
    "        return words_count, total_count\n",
    "    \n",
    "    def load_or_construct_weighted_embedding(self, embedding_fname,\n",
    "                                            embedding_method,\n",
    "                                            embedding_corpus_fname, a=0.0001):\n",
    "        dictionary = {}\n",
    "        if os.path.exists(embedding_fname + '-weighted'):\n",
    "            # load weighted word embeddings\n",
    "            with open(embedding_fname + '-weighted', 'r') as f2:\n",
    "                for line in f2:\n",
    "                    word, weighted_vector = line.strip().split('\\u241E')\n",
    "                    weighted_vector = \\\n",
    "                        [float(el) for el in weighted_vector.split()]\n",
    "                    dictionary[word] = weighted_vector\n",
    "        else:\n",
    "            # load pretrained word embeddings\n",
    "            words, vecs = self.load_word_embeddings(embedding_fname,embedding_method)\n",
    "\n",
    "            # compute word frequency\n",
    "            words_count, total_count = compute_word_frequency(embedding_corpus_fname)\n",
    "            \n",
    "            # construct weighted word embeddings\n",
    "            with open(embeding_fname + '-weighted', 'w') as f3:\n",
    "                for word, vec in zip(words, vecs):\n",
    "                    if word in words_count.keys():\n",
    "                        word_prob = words_count[word] / total_count\n",
    "                    else:\n",
    "                        word_prob = 0.0\n",
    "                    weighted_vector = ( a/ (word_prob + a) ) * np.asarray(vec)\n",
    "                    dictionary[word] = weighted_vector\n",
    "                    f3.writelines(word + '\\u241E' + \" \".join([str(el) for el in weighted_vector]) + \"\\n\")\n",
    "        return dictionary\n",
    "    \n",
    "    def train_model(self, train_data_fname, model_fname):\n",
    "        model = {'vectors':[], 'labels':[], 'sentences':[]}\n",
    "        train_data = self.load_or_tokenized_corpus(train_data_fname)\n",
    "        with open(model_fname, 'w') as f:\n",
    "            for sentence, tokens, label in train_data:\n",
    "                sentence_vector = self.get_sentence_vector(tokens)\n",
    "                model['sentences'].append(sentence)\n",
    "                model['vectors'].append(sentence_vector)\n",
    "                model['labels'].append(label)\n",
    "                str_vector = \" \".join([str(el) for el in sentence_vector])\n",
    "                f.writelines(sentence + '\\u241E' + \" \".join(tokens) + '\\u241E' + str_vector + '\\u241E' + label + '\\n')\n",
    "        return model\n",
    "    \n",
    "    def get_sentence_vector(self, tokens):\n",
    "        vector = np.zeros(self.dim)\n",
    "        for token in tokens:\n",
    "            if token in self.embedding.keys():\n",
    "                vector += self.embeddings[token]\n",
    "        if not self.average:\n",
    "            vector /= len(tokens)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
