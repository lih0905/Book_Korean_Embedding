{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a499472aff01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import argparse, os, sys\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dev\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "\n",
    "def get_tokenizer(tokenizer_name):\n",
    "\n",
    "    tokenizer_dict={\n",
    "        'komoran':Komoran(),\n",
    "        'okt':Okt(),\n",
    "        'mecab':Mecab(),\n",
    "        'hannanum':Hannanum(),\n",
    "        'kkma':Kkma()\n",
    "    }\n",
    "    try:\n",
    "        tokenizer=tokenizer_dict[tokenizer_name]\n",
    "    except:\n",
    "        tokenizer=Mecab()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoWModel(object):\n",
    "    \n",
    "    def __init__(self, train_fname, embedding_fname,\n",
    "                model_fname, embedding_corpus_fname,\n",
    "                embedding_method='word2vec', is_weighted=True,\n",
    "                average=False, dim=100, tokenizer_name='mecab'):\n",
    "        # configurations\n",
    "        #make_save_path(model_fname)\n",
    "        self.dim = dim\n",
    "        self.average = average\n",
    "        if is_weighted:\n",
    "            model_full_fname = model_fname + '-weighted'\n",
    "        else:\n",
    "            model_full_fname = model_fname + '-original'\n",
    "        self.tokenizer = self.get_tokenizer(tokenizer_name)\n",
    "        if is_weighted:\n",
    "            # weighted embeddings\n",
    "            self.embeddings = \\\n",
    "                self.load_or_construct_weighted_embedding(embedding_fname, \n",
    "                                                         embedding_method, embedding_corpus_fname)\n",
    "            print('loading weighted embeddings, complete!')\n",
    "        else:\n",
    "            # original embeddings\n",
    "            words, vectors = self.load_word_embeddings(embedding_fname,embedding_method)\n",
    "            self.embeddings = defaultdict(list)\n",
    "            for word, vector in zip(words, vectors):\n",
    "                self.embeddings[word] = vector\n",
    "            print('loading original embeddings, complete!')\n",
    "        if not os.path.exists(model_full_name):\n",
    "            print('trian Continuous Bag of Words model')\n",
    "            self.model = self.train_model(train_fname, model_full_name)\n",
    "        else:\n",
    "            print('load Continuous Bag of Words model')\n",
    "            self.model = self.load_model(model_full_fname)\n",
    "\n",
    "    def make_save_path(full_path):\n",
    "        #if full_path[:4] == \"data\":\n",
    "        #    full_path = \"/notebooks/embedding/\" + full_path\n",
    "        model_path = '/'.join(full_path.split(\"/\")[:-1])\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "\n",
    "    def get_tokenizer(self, tokenizer_name):\n",
    "        from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "        tokenizer_dict={\n",
    "            'komoran':Komoran(),\n",
    "            'okt':Okt(),\n",
    "            'mecab':Mecab(),\n",
    "            'hannanum':Hannanum(),\n",
    "            'kkma':Kkma()\n",
    "        }\n",
    "        try:\n",
    "            tokenizer=tokenizer_dict(tokenizer_name)\n",
    "        except:\n",
    "            tokenizer=Mecab()\n",
    "        return tokenizer\n",
    "        \n",
    "    def compute_word_frequency(self, embedding_corpus_fname):\n",
    "        total_count = 0\n",
    "        words_count = defaultdict(int)\n",
    "        with open(embedding_corpus_fname, 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                for token in tokens:\n",
    "                    words_count[token] += 1\n",
    "                    total_count += 1\n",
    "        return words_count, total_count\n",
    "    \n",
    "    def load_word_embeddings(self, vecs_fname, method):\n",
    "        if method == 'word2vec':\n",
    "            model = Word2Vec.load(vecs_fname)\n",
    "            words = model.wv.index2word\n",
    "            vecs = model.wv.vectors\n",
    "        else:\n",
    "            words, vecs = [], []\n",
    "            with open(vecs_fname, 'r', encoding='utf-8') as f1:\n",
    "                if 'fasttext' in method:\n",
    "                    next(f1) # skip head line\n",
    "                for line in f1:\n",
    "                    if method == 'swivel':\n",
    "                        splited_line = line.replace('\\n', '').strip().split('\\t')\n",
    "                    else:\n",
    "                        splited_line = line.replace('\\n', '').strip().split(\" \")\n",
    "                    words.append(splited_line[0])\n",
    "                    vec = [float(el) for el in splited_line[1:]]\n",
    "                    vecs.append(vec)\n",
    "            return words, vecs \n",
    "    \n",
    "    def load_or_construct_weighted_embedding(self, embedding_fname,\n",
    "                                            embedding_method,\n",
    "                                            embedding_corpus_fname, a=0.0001):\n",
    "        dictionary = {}\n",
    "        if os.path.exists(embedding_fname + '-weighted'):\n",
    "            # load weighted word embeddings\n",
    "            with open(embedding_fname + '-weighted', 'r') as f2:\n",
    "                for line in f2:\n",
    "                    word, weighted_vector = line.strip().split('\\u241E')\n",
    "                    weighted_vector = \\\n",
    "                        [float(el) for el in weighted_vector.split()]\n",
    "                    dictionary[word] = weighted_vector\n",
    "        else:\n",
    "            # load pretrained word embeddings\n",
    "            words, vecs = self.load_word_embeddings(embedding_fname,embedding_method)\n",
    "\n",
    "            # compute word frequency\n",
    "            words_count, total_count = compute_word_frequency(embedding_corpus_fname)\n",
    "            \n",
    "            # construct weighted word embeddings\n",
    "            with open(embeding_fname + '-weighted', 'w') as f3:\n",
    "                for word, vec in zip(words, vecs):\n",
    "                    if word in words_count.keys():\n",
    "                        word_prob = words_count[word] / total_count\n",
    "                    else:\n",
    "                        word_prob = 0.0\n",
    "                    weighted_vector = ( a/ (word_prob + a) ) * np.asarray(vec)\n",
    "                    dictionary[word] = weighted_vector\n",
    "                    f3.writelines(word + '\\u241E' + \" \".join([str(el) for el in weighted_vector]) + \"\\n\")\n",
    "        return dictionary\n",
    "    \n",
    "    def load_or_tokenize_corpus(self, fname):\n",
    "        data = []\n",
    "        if os.path.exists(fname + \"-tokenized\"):\n",
    "            with open(fname + \"-tokenized\", \"r\") as f1:\n",
    "                for line in f1:\n",
    "                    sentence, tokens, label = line.strip().split(\"\\u241E\")\n",
    "                    data.append([sentence, tokens.split(), label])\n",
    "        else:\n",
    "            with open(fname, \"r\") as f2, open(fname + \"-tokenized\", \"w\") as f3:\n",
    "                for line in f2:\n",
    "                    sentence, label = line.strip().split(\"\\u241E\")\n",
    "                    tokens = self.tokenizer.morphs(sentence)\n",
    "                    data.append([sentence, tokens, label])\n",
    "                    f3.writelines(sentence + \"\\u241E\" + ' '.join(tokens) + \"\\u241E\" + label + \"\\n\")\n",
    "        return data\n",
    "    \n",
    "    def train_model(self, train_data_fname, model_fname):\n",
    "        model = {'vectors':[], 'labels':[], 'sentences':[]}\n",
    "        train_data = self.load_or_tokenized_corpus(train_data_fname)\n",
    "        with open(model_fname, 'w') as f:\n",
    "            for sentence, tokens, label in train_data:\n",
    "                sentence_vector = self.get_sentence_vector(tokens)\n",
    "                model['sentences'].append(sentence)\n",
    "                model['vectors'].append(sentence_vector)\n",
    "                model['labels'].append(label)\n",
    "                str_vector = \" \".join([str(el) for el in sentence_vector])\n",
    "                f.writelines(sentence + '\\u241E' + \" \".join(tokens) + '\\u241E' + str_vector + '\\u241E' + label + '\\n')\n",
    "        return model\n",
    "    \n",
    "    def get_sentence_vector(self, tokens):\n",
    "        vector = np.zeros(self.dim)\n",
    "        for token in tokens:\n",
    "            if token in self.embedding.keys():\n",
    "                vector += self.embeddings[token]\n",
    "        if self.average:\n",
    "            vector /= len(tokens)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "        if vector_norm != 0:\n",
    "            unit_vector = vector / vector_norm\n",
    "        else:\n",
    "            unit_vector = np.zeros(self.dim)\n",
    "        return unit_vector\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenizer.morphs(sentence)\n",
    "        sentence_vector = self.get_sentence_vector(tokens)\n",
    "        scores = np.dot(self.model['vectors'], sentence_vector)\n",
    "        pred = self.model['labels'][np.argmax(scores)]\n",
    "        return pred\n",
    "    \n",
    "    def predict_by_batch(self, tokenized_sentence, labels):\n",
    "        sentence_vectors, eval_score = [], 0\n",
    "        for tokens in tokenized_sentence:\n",
    "            sentence_vectors.append(self.get_sentence_vector(tokens))\n",
    "        scores = np.dot(self.model['vectors'], np.array(sentence_vectors).T)\n",
    "        preds = np.argmax(scores, axis=0)\n",
    "        for pred, label in zip(preds, labels):\n",
    "            if self.model['labels'][pred] == label:\n",
    "                eval_score += 1\n",
    "        return preds, eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CBoWModel(object):\n",
    "\n",
    "    def __init__(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname,\n",
    "                 embedding_method=\"fasttext\", is_weighted=True, average=False, dim=100, tokenizer_name=\"mecab\"):\n",
    "        # configurations\n",
    "        make_save_path(model_fname)\n",
    "        self.dim = dim\n",
    "        self.average = average\n",
    "        if is_weighted:\n",
    "            model_full_fname = model_fname + \"-weighted\"\n",
    "        else:\n",
    "            model_full_fname = model_fname + \"-original\"\n",
    "        self.tokenizer = get_tokenizer(tokenizer_name)\n",
    "        if is_weighted:\n",
    "            # ready for weighted embeddings\n",
    "            self.embeddings = self.load_or_construct_weighted_embedding(embedding_fname, embedding_method, embedding_corpus_fname)\n",
    "            print(\"loading weighted embeddings, complete!\")\n",
    "        else:\n",
    "            # ready for original embeddings\n",
    "            words, vectors = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "            self.embeddings = defaultdict(list)\n",
    "            for word, vector in zip(words, vectors):\n",
    "                self.embeddings[word] = vector\n",
    "            print(\"loading original embeddings, complete!\")\n",
    "        if not os.path.exists(model_full_fname):\n",
    "            print(\"train Continuous Bag of Words model\")\n",
    "            self.model = self.train_model(train_fname, model_full_fname)\n",
    "        else:\n",
    "            print(\"load Continuous Bag of Words model\")\n",
    "            self.model = self.load_model(model_full_fname)\n",
    "\n",
    "    def evaluate(self, test_data_fname, batch_size=3000, verbose=False):\n",
    "        print(\"evaluation start!\")\n",
    "        test_data = self.load_or_tokenize_corpus(test_data_fname)\n",
    "        data_size = len(test_data)\n",
    "        num_batches = int((data_size - 1) / batch_size) + 1\n",
    "        eval_score = 0\n",
    "        for batch_num in range(num_batches):\n",
    "            batch_sentences = []\n",
    "            batch_tokenized_sentences = []\n",
    "            batch_labels = []\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            features = test_data[start_index:end_index]\n",
    "            for feature in features:\n",
    "                sentence, tokens, label = feature\n",
    "                batch_sentences.append(sentence)\n",
    "                batch_tokenized_sentences.append(tokens)\n",
    "                batch_labels.append(label)\n",
    "            preds, curr_eval_score = self.predict_by_batch(batch_tokenized_sentences, batch_labels)\n",
    "            eval_score += curr_eval_score\n",
    "        if verbose:\n",
    "            for sentence, pred, label in zip(batch_sentences, preds, batch_labels):\n",
    "                print(sentence, \", pred:\", pred, \", label:\", label)\n",
    "        print(\"# of correct:\", str(eval_score), \", total:\", str(len(test_data)), \", score:\", str(eval_score / len(test_data)))\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenizer.morphs(sentence)\n",
    "        sentence_vector = self.get_sentence_vector(tokens)\n",
    "        scores = np.dot(self.model[\"vectors\"], sentence_vector)\n",
    "        pred = self.model[\"labels\"][np.argmax(scores)]\n",
    "        return pred\n",
    "\n",
    "    def predict_by_batch(self, tokenized_sentences, labels):\n",
    "        sentence_vectors, eval_score = [], 0\n",
    "        for tokens in tokenized_sentences:\n",
    "            sentence_vectors.append(self.get_sentence_vector(tokens))\n",
    "        scores = np.dot(self.model[\"vectors\"], np.array(sentence_vectors).T)\n",
    "        preds = np.argmax(scores, axis=0)\n",
    "        for pred, label in zip(preds, labels):\n",
    "            if self.model[\"labels\"][pred] == label:\n",
    "                eval_score += 1\n",
    "        return preds, eval_score\n",
    "\n",
    "    def get_sentence_vector(self, tokens):\n",
    "        vector = np.zeros(self.dim)\n",
    "        for token in tokens:\n",
    "            if token in self.embeddings.keys():\n",
    "                vector += self.embeddings[token]\n",
    "        if not self.average:\n",
    "            vector /= len(tokens)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "        if vector_norm != 0:\n",
    "            unit_vector = vector / vector_norm\n",
    "        else:\n",
    "            unit_vector = np.zeros(self.dim)\n",
    "        return unit_vector\n",
    "\n",
    "    def load_or_tokenize_corpus(self, fname):\n",
    "        data = []\n",
    "        if os.path.exists(fname + \"-tokenized\"):\n",
    "            with open(fname + \"-tokenized\", \"r\") as f1:\n",
    "                for line in f1:\n",
    "                    sentence, tokens, label = line.strip().split(\"\\u241E\")\n",
    "                    data.append([sentence, tokens.split(), label])\n",
    "        else:\n",
    "            with open(fname, \"r\") as f2, open(fname + \"-tokenized\", \"w\") as f3:\n",
    "                for line in f2:\n",
    "                    sentence, label = line.strip().split(\"\\u241E\")\n",
    "                    tokens = self.tokenizer.morphs(sentence)\n",
    "                    data.append([sentence, tokens, label])\n",
    "                    f3.writelines(sentence + \"\\u241E\" + ' '.join(tokens) + \"\\u241E\" + label + \"\\n\")\n",
    "        return data\n",
    "\n",
    "    def compute_word_frequency(self, embedding_corpus_fname):\n",
    "        total_count = 0\n",
    "        words_count = defaultdict(int)\n",
    "        with open(embedding_corpus_fname, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                for token in tokens:\n",
    "                    words_count[token] += 1\n",
    "                    total_count += 1\n",
    "        return words_count, total_count\n",
    "\n",
    "    def load_word_embeddings(self, vecs_fname, method):\n",
    "        if method == \"word2vec\":\n",
    "            model = Word2Vec.load(vecs_fname)\n",
    "            words = model.wv.index2word\n",
    "            vecs = model.wv.vectors\n",
    "        else:\n",
    "            words, vecs = [], []\n",
    "            with open(vecs_fname, 'r', encoding='utf-8', ) as f1:\n",
    "                if \"fasttext\" in method:\n",
    "                    next(f1)  # skip head line\n",
    "                for line in f1:\n",
    "                    if method == \"swivel\":\n",
    "                        splited_line = line.replace(\"\\n\", \"\").strip().split(\"\\t\")\n",
    "                    else:\n",
    "                        splited_line = line.replace(\"\\n\", \"\").strip().split(\" \")\n",
    "                    words.append(splited_line[0])\n",
    "                    vec = [float(el) for el in splited_line[1:]]\n",
    "                    vecs.append(vec)\n",
    "        return words, vecs\n",
    "\n",
    "    def load_or_construct_weighted_embedding(self, embedding_fname, embedding_method, embedding_corpus_fname, a=0.0001):\n",
    "        dictionary = {}\n",
    "        if os.path.exists(embedding_fname + \"-weighted\"):\n",
    "            # load weighted word embeddings\n",
    "            with open(embedding_fname + \"-weighted\", \"r\") as f2:\n",
    "                for line in f2:\n",
    "                    word, weighted_vector = line.strip().split(\"\\u241E\")\n",
    "                    weighted_vector = [float(el) for el in weighted_vector.split()]\n",
    "                    dictionary[word] = weighted_vector\n",
    "        else:\n",
    "            # load pretrained word embeddings\n",
    "            words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)\n",
    "            # compute word frequency\n",
    "            words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname)\n",
    "            # construct weighted word embeddings\n",
    "            with open(embedding_fname + \"-weighted\", \"w\") as f3:\n",
    "                for word, vec in zip(words, vecs):\n",
    "                    if word in words_count.keys():\n",
    "                        word_prob = words_count[word] / total_word_count\n",
    "                    else:\n",
    "                        word_prob = 0.0\n",
    "                    weighted_vector = (a / (word_prob + a)) * np.asarray(vec)\n",
    "                    dictionary[word] = weighted_vector\n",
    "                    f3.writelines(word + \"\\u241E\" + \" \".join([str(el) for el in weighted_vector]) + \"\\n\")\n",
    "        return dictionary\n",
    "\n",
    "    def train_model(self, train_data_fname, model_fname):\n",
    "        model = {\"vectors\": [], \"labels\": [], \"sentences\": []}\n",
    "        train_data = self.load_or_tokenize_corpus(train_data_fname)\n",
    "        with open(model_fname, \"w\") as f:\n",
    "            for sentence, tokens, label in train_data:\n",
    "                tokens = self.tokenizer.morphs(sentence)\n",
    "                sentence_vector = self.get_sentence_vector(tokens)\n",
    "                model[\"sentences\"].append(sentence)\n",
    "                model[\"vectors\"].append(sentence_vector)\n",
    "                model[\"labels\"].append(label)\n",
    "                str_vector = \" \".join([str(el) for el in sentence_vector])\n",
    "                f.writelines(sentence + \"\\u241E\" + \" \".join(tokens) + \"\\u241E\" + str_vector + \"\\u241E\" + label + \"\\n\")\n",
    "        return model\n",
    "\n",
    "    def load_model(self, model_fname):\n",
    "        model = {\"vectors\": [], \"labels\": [], \"sentences\": []}\n",
    "        with open(model_fname, \"r\") as f:\n",
    "            for line in f:\n",
    "                sentence, _, vector, label = line.strip().split(\"\\u241E\")\n",
    "                vector = np.array([float(el) for el in vector.split()])\n",
    "                model[\"sentences\"].append(sentence)\n",
    "                model[\"vectors\"].append(vector)\n",
    "                model[\"labels\"].append(label)\n",
    "        return model\n",
    "\n",
    "\n",
    "def make_save_path(full_path):\n",
    "#    if full_path[:4] == \"data\":\n",
    "#        full_path = \"/notebooks/embedding/\" + full_path\n",
    "    model_path = '/'.join(full_path.split(\"/\")[:-1])\n",
    "    if not os.path.exists(model_path):\n",
    "       os.makedirs(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = 'data/processed/processed_ratings_train.txt'\n",
    "embedding_fname = 'data/word-embeddings/word2vec/word2vec'\n",
    "model_fname = 'data/word-embeddings/cbow/word2vec'\n",
    "embedding_corpus_fname = 'data/tokenized/corpus_mecab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'h\\x0e)q!}q\"(h\\x11M\\x14h\\x12M\\t\\x0ch\\x13\\x05\\x00\\x00\\x00\\x00\\x01ubX\\x01\\x00\\x00\\x00(q#h\\x0e)q$}q%(h\\x11Ji6\\x00h\\x12K'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-352efb42e88c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBoWModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_corpus_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-11fbaa7d8c08>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname, embedding_method, is_weighted, average, dim, tokenizer_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_weighted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# ready for weighted embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_or_construct_weighted_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_corpus_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading weighted embeddings, complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-11fbaa7d8c08>\u001b[0m in \u001b[0;36mload_or_construct_weighted_embedding\u001b[0;34m(self, embedding_fname, embedding_method, embedding_corpus_fname, a)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# load pretrained word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;31m# compute word frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mwords_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_word_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_word_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_corpus_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-11fbaa7d8c08>\u001b[0m in \u001b[0;36mload_word_embeddings\u001b[0;34m(self, vecs_fname, method)\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0msplited_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplited_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplited_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                     \u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-11fbaa7d8c08>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0msplited_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplited_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplited_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                     \u001b[0mvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'h\\x0e)q!}q\"(h\\x11M\\x14h\\x12M\\t\\x0ch\\x13\\x05\\x00\\x00\\x00\\x00\\x01ubX\\x01\\x00\\x00\\x00(q#h\\x0e)q$}q%(h\\x11Ji6\\x00h\\x12K'"
     ]
    }
   ],
   "source": [
    "dd = CBoWModel(train_fname, embedding_fname,model_fname, embedding_corpus_fname,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python models/word_utils.py \\\n",
    "--train_corpus_path data/processed/processed_ratings_train.txt \\\n",
    "--test_corpus_path data/processed/processed_ratings_test.txt \\\n",
    "--embedding_path data/word-embeddings/word2vec/word2vec \\\n",
    "--output_path data/word-embeddings/cbow/word2vec \\\n",
    "--embedding_name word2vec --method cbow --is_weighted False\n",
    "\n",
    "> \\# of correct: 36485 , total: 49997 , score: 0.7297437846270777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python models/word_utils.py \\\n",
    "--train_corpus_path data/processed/processed_ratings_train.txt \\\n",
    "--test_corpus_path data/processed/processed_ratings_test.txt \\\n",
    "--embedding_corpus_path data/tokenized/corpus_mecab.txt \\\n",
    "--embedding_path data/word-embeddings/word2vec/word2vec \\\n",
    "--output_path data/word-embeddings/cbow/word2vec \\\n",
    "--embedding_name word2vec --method cbow --is_weighted True\n",
    "\n",
    "> \\# of correct: 36742 , total: 49997 , score: 0.7348840930455828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
