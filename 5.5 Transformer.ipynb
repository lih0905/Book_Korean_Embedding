{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harvard NLP 의 [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)를 구현하는 것을 목표로 해보자.\n",
    "\n",
    "# Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context='talk')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.srd_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mak):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./transformer.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Stacks\n",
    "\n",
    "### Encoder\n",
    "\n",
    "인코더는 $N=6$개의 동일한 레이어를 쌓아서 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual connection과 layer normailzation을 도입한다. 또한 각각의 sublayer에 dropout을 도입한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9153, 0.6661, 0.5379, 0.0969],\n",
       "        [0.5634, 0.8127, 0.7820, 0.4595]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5541, 0.6544]), tensor([[0.5541],\n",
       "         [0.6544]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(-1, keepdim=False), a.mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]), torch.Size([2]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size(), a.mean(-1, keepdim=False).size(), a.mean(-1, keepdim=True).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module.\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x-mean) / (std+self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as oppposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        # 처음 시작 단계에도 normalization이 필요한가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 레이어는 두 개의 서브레이어를 포함한다. 첫번째는 멀티헤드 셀프어텐션이고 두번째는 단순한 위치별 FC 레이어이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Follow the above figure (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "디코더도 비슷함..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더에 비해 디코더는 하나의 서브레이어가 더 필요하며, 여기서는 인코더의 아웃풋을 받아서 멀티헤드 셀프어텐션을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow the above figure (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x,m,m,src_mask))\n",
    "        return self.sublayers[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent postions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy.triu($m, k=0$)\n",
    "Upper triangle of an array. Return a copy of a matrix with the elements below the $k$-th diagonal zeroed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]]), array([[1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.]]), array([[0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(np.ones([4,4]), k=1), np.triu(np.ones([4,4]), k=0), np.triu(np.ones([4,4]), k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None\n",
    "# 각각의 타겟 단어(행)가 참조할 수 있는 범위(열)을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
