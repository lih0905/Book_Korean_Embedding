{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도 학습 기반 형태소 분석\n",
    "## KoNLPy 형태소 분석기 활용\n",
    "\n",
    "- Mecab (a.k.a 은전한닢)을 활용한 형태소 분석이 가장 대중적인 Tokenization 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '가', '방', '에', '들어가', '신다']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "tokenizer.morphs('아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아버지 가 방 에 들어가 신다'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokenizer.morphs('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `morphs` function을 이용하면 형태소 단위의 **tokenization**이 가능하고,\n",
    "- `pos` function을 이용하면 각 형태소의 **POS (Part-Of-Speech)** Tag를 형태소와 함께 출력 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('방', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('신다', 'EP+EC')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pos('아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교재에서는 KoNLPy에 내장되어 있는 다양한 형태소 분석기를 유연하게 사용하기 위해 `get_tokenizer` 함수를 정의하였으나,\n",
    "- `Mecab`을 사용할 수 있는 환경이라면 굳이 다른 형태소 분석기를 사용할 이유가 없는 것으로 알고 있음\n",
    "\n",
    "> +) **조건문**으로 인한 분기를 최대한 피하고 의식적으로 **Key access**를 지향하자!\n",
    ">\n",
    "> _github repository 내 [slide](https://github.com/nlp-kkmas/korean-embedding-study/blob/master/treasure/Clean%20code%20for%20AI%2C%20ML.pdf) 참고_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "\n",
    "def get_tokenizer(name):\n",
    "    key = {'komoran': Komoran(),\n",
    "           'okt': Okt(),\n",
    "           'mecab': Mecab(),\n",
    "           'hannanum': Hannanum(),\n",
    "           'kkma': Kkma()}\n",
    "    \n",
    "    if name not in key:\n",
    "        name = 'mecab'\n",
    "    \n",
    "    return key[name]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기정의한 `get_tokenizer` function을 이용해 형태소 분석기를 생성하는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']\n",
      "[('아버지', 'NNG'), ('가방', 'NNP'), ('에', 'JKB'), ('들어가', 'VV'), ('시', 'EP'), ('ㄴ다', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('komoran')\n",
    "print(tokenizer.morphs('아버지가방에들어가신다'))\n",
    "print(tokenizer.pos('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'konlpy.tag._mecab.Mecab'>\n",
      "['아버지', '가', '방', '에', '들어가', '신다']\n",
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EC')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('bullshit')\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.morphs('아버지가방에들어가신다'))\n",
    "print(tokenizer.pos('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> KoNLPy 내 형태소 분석기 간 비교는 다음 글을 참조하자 ! : [누가누가 잘하나! 대화체와 합이 잘 맞는 Tokenizer를 찾아보자!](https://blog.pingpong.us/tokenizer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khaiii 형태소 분석기 활용\n",
    "\n",
    "- MacOS, linux 환경에서는 카카오가 제작한 `Khaiii` 형태소 분석기를 활용 가능\n",
    "- Mecab 등과 달리 자연어 처리에 적용할 경우, **원문을 복원**하는 과정이 추가적으로 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-53e2b54a174f>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-53e2b54a174f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    print(f'Tokens with morpheme information: {tokens}')\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "\n",
    "tokenizer = KhaiiiApi()\n",
    "data = tokenizer.analyze(\"아버지가방에들어가신다\")\n",
    "\n",
    "tokens = []\n",
    "clean_tokens = []\n",
    "\n",
    "for word in data:\n",
    "    tokens.extend([m.lex for m in word.morphs])\n",
    "    clean_tokens.extend([(m.lex, m.tag) for m in word.morphs])\n",
    "\n",
    "print(f'Tokens with morpheme information: {tokens}')\n",
    "print(f'Tokens without morpheme information: {clean_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 사전 추가\n",
    "\n",
    "- 진행하는 프로젝트에 따라 기학습된 규칙을 기반으로 형태소 분석을 수행하는 KoNLPy 형태소 분석기들에 **형태소로 분리되지 않아야 하는** 사용자 정의 단어를 추가해줄 필요가 있을 수 있음\n",
    "- 이럴 때 **사용자 사전**을 정의해 각 형태소 분석기에 추가해줄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가우스', '전자', '텔레비전', '정말', '좋', '네요']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad case:\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "tokenizer.morphs('가우스전자 텔레비전 정말 좋네요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('가우스', 'NNP'),\n",
       " ('전자', 'NNG'),\n",
       " ('텔레비전', 'NNG'),\n",
       " ('정말', 'MAG'),\n",
       " ('좋', 'VA'),\n",
       " ('네요', 'EC')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pos('가우스전자 텔레비전 정말 좋네요') # 가우스전자를 하나의 독립된 고유명사로 추가해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표층형/0/0/0/품사태그/의미분류/종성유무/읽기/타입/첫번째품사/마지막품사/표현 ([link](https://kugancity.tistory.com/entry/mecab%EC%97%90-%EC%82%AC%EC%9A%A9%EC%9E%90%EC%82%AC%EC%A0%84%EA%B8%B0%EB%B6%84%EC%84%9D-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0))\n",
    "\n",
    "> 가우스전자,,,,NNP,**,T,가우스전자,**,**,**,**,**\n",
    "\n",
    "> bash preprocess.sh [mecab-user-dic](https://github.com/ratsgo/embedding/blob/master/preprocess.sh#L195) 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비지도 학습 기반 형태소 분석\n",
    "\n",
    "## soynlp\n",
    "- 형태소 분석, 품사 판별 등을 지원하는 한국어 자연어 처리 패키지\n",
    "- 하나의 문장/문서에서보다는 어느 정도 규모가 있으면서 동질적인 문서 집합에서 잘 작동\n",
    "- soynlp의 학습 지표\n",
    "    1. **Cohesion Probability**: 주어진 문자열이 유기적으로 연결돼 함께 자주 나타나는가?\n",
    "    2. **Branching Entropy**: 해당 단어 앞뒤로 다양한 조사, 어미 혹은 다른 단어가 등장하는가?\n",
    "- soynlp와 같은 비지도 학습 기반의 library는 corpus를 바탕으로 한 학습 이후 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soynlp 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.745 Gbse memory 0.541 Gb\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "\n",
    "corpus_fname = 'data/processed_ratings.txt'\n",
    "model_fname = 'data/soyword.model'\n",
    "\n",
    "sentences = [sent.strip() for sent in open(corpus_fname, 'r').readlines()]\n",
    "word_extractor = WordExtractor(min_frequency=100,\n",
    "                               min_cohesion_forward=0.05,\n",
    "                               min_right_branching_entropy=0.0)\n",
    "word_extractor.train(sentences)\n",
    "word_extractor.save(model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습된 soynlp 모델의 점수를 이용한 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 6130\n",
      "all branching entropies was computed # words = 123575\n",
      "all accessor variety was computed # words = 123575\n",
      "['애비는종이었다']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "model_fname = 'data/soyword.model'\n",
    "word_extractor = WordExtractor(min_frequency=100,\n",
    "                               min_cohesion_forward=0.05,\n",
    "                               min_right_branching_entropy=0.0)\n",
    "word_extractor.load(model_fname)\n",
    "\n",
    "scores = word_extractor.word_scores()\n",
    "scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}\n",
    "\n",
    "tokenizer = LTokenizer(scores=scores)\n",
    "tokens = tokenizer.tokenize(\"애비는 종이었다\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google SentencePiece\n",
    "- Google에서 공개한 BPE (Byte Pair Encoding) 기반의 Tokenizer\n",
    "- Corpus에서 가장 많이 등장한 문자열을 병합해 압축하여 사용\n",
    "- Goal: 원하는 Vocab size를 갖출 때 까지 반복적으로 고빈도 문자열을 병합해 Vocab에 추가 !\n",
    "    - 띄어쓰기를 기준으로 나눈 어절에 Sub-word가 포함되어 있는 경우 **최장 길이 일치**를 기준으로 Subword를 분리\n",
    "    - Subword 탐색을 반복한 후, Vocab에 없는 단어를 만나는 경우 **OOV** (Out Of Vocabulary) 처리\n",
    "- **BERT**를 비롯한 다양한 신모델들은 BPE 기반의 Tokenizer를 사용\n",
    "    - 사실상 **Transformer**의 등장 이후, 영어권에서는 **BPE Tokenization**이 **최선책**으로 등극"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece 학습 코드 (don't run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "train = '--input=data/processed_wiki_ko.txt --model_prefix=sentpiece --vocab_size=32000 --model_type=bpe --character_coverage=0.9995'\n",
    "spm.SentencePieceTrainer.Train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE 기반의 BERT Tokenizer (using. special tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version               \n",
      "------------------- ----------------------\n",
      "absl-py             0.6.1                 \n",
      "astor               0.7.1                 \n",
      "atomicwrites        1.3.0                 \n",
      "attrs               19.1.0                \n",
      "backcall            0.1.0                 \n",
      "bleach              3.0.2                 \n",
      "bokeh               1.2.0                 \n",
      "boto                2.49.0                \n",
      "boto3               1.9.180               \n",
      "botocore            1.12.180              \n",
      "certifi             2019.6.16             \n",
      "chardet             3.0.4                 \n",
      "cmake               3.14.4                \n",
      "cycler              0.10.0                \n",
      "decorator           4.3.0                 \n",
      "defusedxml          0.5.0                 \n",
      "docutils            0.14                  \n",
      "entrypoints         0.2.3                 \n",
      "fasttext            0.9                   \n",
      "funcy               1.12                  \n",
      "future              0.17.1                \n",
      "gast                0.2.0                 \n",
      "gensim              3.7.3                 \n",
      "grpcio              1.16.0                \n",
      "h5py                2.8.0                 \n",
      "idna                2.8                   \n",
      "importlib-metadata  0.18                  \n",
      "ipykernel           5.1.0                 \n",
      "ipython             7.1.1                 \n",
      "ipython-genutils    0.2.0                 \n",
      "ipywidgets          7.4.2                 \n",
      "jedi                0.13.1                \n",
      "Jinja2              2.10                  \n",
      "jmespath            0.9.4                 \n",
      "joblib              0.13.2                \n",
      "JPype1              0.7.0                 \n",
      "jsonschema          2.6.0                 \n",
      "jupyter             1.0.0                 \n",
      "jupyter-client      5.2.3                 \n",
      "jupyter-console     6.0.0                 \n",
      "jupyter-core        4.4.0                 \n",
      "Keras-Applications  1.0.6                 \n",
      "Keras-Preprocessing 1.0.5                 \n",
      "khaiii              0.4                   \n",
      "kiwisolver          1.0.1                 \n",
      "konlpy              0.5.1                 \n",
      "lxml                4.3.4                 \n",
      "Markdown            3.0.1                 \n",
      "MarkupSafe          1.1.0                 \n",
      "matplotlib          3.0.1                 \n",
      "mecab-python        0.996-ko-0.9.2        \n",
      "mistune             0.8.4                 \n",
      "more-itertools      7.1.0                 \n",
      "nbconvert           5.4.0                 \n",
      "nbformat            4.4.0                 \n",
      "networkx            2.3                   \n",
      "notebook            5.7.0                 \n",
      "numexpr             2.6.9                 \n",
      "numpy               1.15.4                \n",
      "packaging           19.0                  \n",
      "pandas              0.23.4                \n",
      "pandocfilters       1.4.2                 \n",
      "parso               0.3.1                 \n",
      "pathlib2            2.3.4                 \n",
      "pexpect             4.6.0                 \n",
      "pickleshare         0.7.5                 \n",
      "Pillow              5.3.0                 \n",
      "pip                 19.1.1                \n",
      "pluggy              0.12.0                \n",
      "prometheus-client   0.4.2                 \n",
      "prompt-toolkit      2.0.7                 \n",
      "protobuf            3.6.1                 \n",
      "psutil              5.6.3                 \n",
      "ptyprocess          0.6.0                 \n",
      "py                  1.8.0                 \n",
      "pybind11            2.3.0                 \n",
      "pycurl              7.43.0                \n",
      "Pygments            2.2.0                 \n",
      "pygobject           3.20.0                \n",
      "pyLDAvis            2.1.2                 \n",
      "pyparsing           2.3.0                 \n",
      "pytest              5.0.0                 \n",
      "python-apt          1.1.0b1+ubuntu0.16.4.2\n",
      "python-dateutil     2.7.5                 \n",
      "pytz                2018.7                \n",
      "PyYAML              5.1.1                 \n",
      "pyzmq               17.1.2                \n",
      "qtconsole           4.4.2                 \n",
      "requests            2.22.0                \n",
      "s3transfer          0.2.1                 \n",
      "scikit-learn        0.20.0                \n",
      "scipy               1.1.0                 \n",
      "selenium            3.141.0               \n",
      "Send2Trash          1.5.0                 \n",
      "sentencepiece       0.1.82                \n",
      "setuptools          40.5.0                \n",
      "six                 1.11.0                \n",
      "sklearn             0.0                   \n",
      "smart-open          1.8.4                 \n",
      "soynlp              0.0.492               \n",
      "soyspacing          1.0.15                \n",
      "tensorboard         1.12.0                \n",
      "tensorflow          1.12.0                \n",
      "termcolor           1.1.0                 \n",
      "terminado           0.8.1                 \n",
      "testpath            0.4.2                 \n",
      "torch               1.3.0+cpu             \n",
      "torchvision         0.4.1+cpu             \n",
      "tornado             5.1.1                 \n",
      "traitlets           4.3.2                 \n",
      "urllib3             1.25.3                \n",
      "wcwidth             0.1.7                 \n",
      "webencodings        0.5.1                 \n",
      "Werkzeug            0.14.1                \n",
      "wheel               0.32.2                \n",
      "widgetsnbextension  3.4.2                 \n",
      "zipp                0.5.1                 \n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5d1831bfd582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/bert.vocab'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'bert'"
     ]
    }
   ],
   "source": [
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "vocab_fname = 'data/bert.vocab'\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)\n",
    "\n",
    "print(tokenizer.tokenize('집에좀 가자'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 띄어쓰기 교정 모델 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soyspacing.countbase import CountSpace\n",
    "\n",
    "corpus_fname = 'data/processed_ratings.txt'\n",
    "model_fname = 'data/space-correct.model'\n",
    "\n",
    "model = CountSpace()\n",
    "model.train(corpus_fname)\n",
    "model.save_model(model_fname, json_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 띄어쓰기 교정 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('어릴때 보고 지금 다시봐도 재밌어요', [0, 0, 1, 0, 1, 0, 1, 0, None, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soyspacing.countbase import CountSpace\n",
    "\n",
    "model_fname = 'data/space-correct.model'\n",
    "model = CountSpace()\n",
    "model.load_model(model_fname, json_format=False)\n",
    "model.correct('어릴때보고 지금다시봐도 재밌어요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참조. MaxScoreTokenizer\n",
    "띄어쓰기가 제대로 지켜지지 않은 데이터라면, 문장의 띄어쓰기 기준으로 나뉘어진 단위가 L + [R] 구조라 가정할 수 없습니다. 하지만 사람은 띄어쓰기가 지켜지지 않은 문장에서 익숙한 단어부터 눈에 들어옵니다. 이 과정을 모델로 옮긴 MaxScoreTokenizer 역시 단어 점수를 이용합니다.\n",
    "\n",
    "```\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "scores = {'파스': 0.3, '파스타': 0.7, '좋아요': 0.2, '좋아':0.5}\n",
    "tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "\n",
    "print(tokenizer.tokenize('난파스타가좋아요'))\n",
    "# ['난', '파스타', '가', '좋아', '요']\n",
    "\n",
    "print(tokenizer.tokenize('난파스타가 좋아요', flatten=False))\n",
    "# [[('난', 0, 1, 0.0, 1), ('파스타', 1, 4, 0.7, 3),  ('가', 4, 5, 0.0, 1)],\n",
    "#  [('좋아', 0, 2, 0.5, 2), ('요', 2, 3, 0.0, 1)]]\n",
    "```\n",
    "\n",
    "MaxScoreTokenizer 역시 WordExtractor 의 결과를 이용하실 때에는 위의 예시처럼 적절히 scores 를 만들어 사용합니다. 이미 알려진 단어 사전이 있다면 이 단어들은 다른 어떤 단어보다도 더 큰 점수를 부여하면 그 단어는 토크나이저가 하나의 단어로 잘라냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 구어체에 띄어쓰기를 적용하고 싶을 경우, [Ping-pong's chatspace](https://github.com/pingpong-ai/chatspace) 참조 !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
