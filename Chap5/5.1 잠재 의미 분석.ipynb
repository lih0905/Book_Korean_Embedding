{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF(term frequency) : 어떤 단어가 특정 문서에서 얼마나 많이 쓰였는지 나타내는 빈도\n",
    "* DF(document frequency) : 특정 단어가 나타난 문서의 수\n",
    "* IDF(inverse document frequency) : 전체 문서 수$(N)$를 해당 단어의 DF로 나눈 뒤 로그를 취한 값. 클 수록 특이한 단어라는 의미\n",
    "\n",
    "$$ \\text{TF-IDF}(w) = \\text{TF}(w) \\times \\log\\frac{N}{\\text{DF}(w)} $$\n",
    "\n",
    "어떤 단어의 주제 예측 능력이 강할 수록 TF-IDF값이 커지고 그 반대의 경우 작아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inhwan/Python/Pytorch_ev/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/inhwan/Python/Pytorch_ev/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/inhwan/Python/Pytorch_ev/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/inhwan/Python/Pytorch_ev/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/inhwan/Python/Pytorch_ev/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/inhwan/Python/Pytorch_ev/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from preprocess import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fname = 'data/processed/processed_blog.txt'\n",
    "tokenizer = get_tokenizer('mecab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, raw_corpus, noun_corpus = [], [], []\n",
    "with open(corpus_fname, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            title, document = line.strip().split('\\u241E')\n",
    "            titles.append(title)\n",
    "            raw_corpus.append(document)\n",
    "            nouns = tokenizer.nouns(document)\n",
    "            noun_corpus.append(' '.join(nouns))\n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 이번 글에서는 최대엔트로피모델(Maximum Entropy model)의 파라메터 추정을 살펴보도록 하겠습니다. 이 글은 기본적으로 [이곳]()을 참고하였습니다. 그럼 시작하겠습니다.   ## 모델 정의 최대엔트로피 모델은 다음과 같이 정의됩니다.  $$ { P }_{ \\\\Lambda }(y|x)=\\\\frac { { exp( }\\\\sum _{ i }^{ }{ { \\\\lambda }_{ i }{ f }_{ i }\\\\left( x,y \\\\right) } ) }{ \\\\sum _{ y }^{ }{ { exp( }\\\\sum _{ i }^{ }{ { \\\\l'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[0][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이번 글 최대 엔트로피 모델 파라 메터 추정 글 기본 이곳 참고 시작 모델 정의 최대 엔트로피 모델 다음 정의 위 식 때 값 반환 함수 자질 벡터 번 값 중요 가중치 요소 가중치 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_corpus[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 TfidfVectorizer를 이용하여 생성한 말뭉치의 명사들에 대하여 TF-IDF 행렬을 생성하자.\n",
    "\n",
    "TfidfVectorizer의 옵션에 대해서는 다음 글을 참고.<br>\n",
    "https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=1, # document frequency가 1 이상\n",
    "    ngram_range=(1,1), # 좌우 (1,1)개씩 고려\n",
    "    lowercase=True,\n",
    "    tokenizer=lambda x: x.split())\n",
    "input_matrix = vectorizer.fit_transform(noun_corpus) # 명사들에 대한 TF-IDF 행렬 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<204x37143 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 76870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 37143)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix.shape # 행은 문서, 열은 단어에 대응"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vocab = {vectorizer.vocabulary_[token]:token\n",
    "           for token in vectorizer.vocabulary_.keys()}\n",
    "# curr_doc : 말뭉치 첫 번째 문서의 TF-IDF 행렬\n",
    "curr_doc, result = input_matrix[0], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([30054, 21719, 17148, 26014, 33661, 20188, 19879, 23540, 22470,\n",
       "        27861], dtype=int32),\n",
       " array([0.02321873, 0.06195969, 0.05037386, 0.06943096, 0.08838695,\n",
       "        0.03324375, 0.03535159, 0.09099347, 0.02587359, 0.03601874]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_doc.indices[:10], curr_doc.data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_doc에서 TF-IDF값이 0이 아닌 요소들을 내림차순 정렬\n",
    "for idx, el in zip(curr_doc.indices, curr_doc.data):\n",
    "    result.append((id2vocab[idx], el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('점', 0.02321872527560292),\n",
       " ('뺄셈', 0.06195969255574101),\n",
       " ('덧셈', 0.05037385883988076),\n",
       " ('업데이트', 0.06943095628902349),\n",
       " ('터', 0.08838695358356398),\n",
       " ('방향', 0.033243751333481585),\n",
       " ('반대', 0.03535158640207159),\n",
       " ('손실', 0.09099347000775752),\n",
       " ('생각', 0.02587358775480135),\n",
       " ('유사', 0.036018736356608516)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('우도', 0.30935433754247393),\n",
       " ('최대', 0.2644197269001561),\n",
       " ('모델', 0.21509543930315736),\n",
       " ('디언', 0.20954601175351925),\n",
       " ('엔트로피', 0.20954601175351925),\n",
       " ('트', 0.2020801317026838),\n",
       " ('메터', 0.18998546457990625),\n",
       " ('파라', 0.18998546457990625),\n",
       " ('확률분포', 0.17931834019736734),\n",
       " ('디센트', 0.1740779030970291)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(result, key=lambda x:x[1], reverse=True)[:10] # 각 문서를 대표하는 단어들이라고 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word2vec'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x37143 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 106 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 생성된 (204, 37413) 크기의 TF-IDF 행렬에 SVD를 수행하여 (204, 100) 크기의 밀집 행렬을 얻자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "vecs = svd.fit_transform(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.86747266e-01, -9.56053051e-02, -3.27831811e-01, -9.30603933e-02,\n",
       "       -1.52408165e-01,  5.70177265e-02,  1.82468966e-01, -6.12313432e-02,\n",
       "       -1.60263310e-01,  1.18883749e-01,  3.32639479e-02,  1.05234070e-01,\n",
       "        7.58770191e-02,  4.28242403e-02, -1.16404879e-01,  6.34770148e-02,\n",
       "       -1.64026660e-02,  3.48644679e-02, -2.30410757e-01, -1.06132091e-01,\n",
       "        1.57809800e-01, -1.09577114e-01, -3.96189500e-02, -1.51988827e-01,\n",
       "       -1.04899695e-02, -7.12179941e-02, -5.24978097e-02,  7.59890032e-02,\n",
       "       -4.45468611e-02,  1.58693679e-01,  9.37446885e-03,  1.55045926e-02,\n",
       "       -1.51671074e-02, -1.23778800e-01,  2.68083345e-02, -7.25614748e-02,\n",
       "       -6.73392994e-02, -4.01429908e-03,  1.09203502e-02, -3.03559219e-02,\n",
       "       -1.55954288e-02, -6.47874112e-03,  2.15010784e-02, -2.59279452e-02,\n",
       "        6.12138387e-02, -1.36160381e-03, -4.81503075e-03,  1.68810019e-02,\n",
       "       -6.51446377e-03,  1.71955208e-02, -1.44910331e-02,  1.75601136e-02,\n",
       "       -6.52873170e-02, -1.36269249e-02, -1.77702115e-02,  5.71368274e-03,\n",
       "       -2.01924488e-02, -1.55776006e-02, -4.53697803e-03,  4.93459551e-02,\n",
       "        4.98238709e-02,  1.55835108e-02, -7.55315404e-03,  3.96007502e-02,\n",
       "       -6.50829139e-02, -5.65373181e-03,  3.33541078e-02, -4.28590856e-02,\n",
       "        1.27885672e-02,  3.84203211e-02,  3.16335500e-02, -7.92102812e-02,\n",
       "       -2.70796042e-04, -1.56874255e-02, -7.20526944e-03, -7.94971743e-03,\n",
       "       -2.03599792e-02,  5.53180734e-02,  6.63550381e-03,  6.38614999e-02,\n",
       "        2.66893826e-02, -1.23080240e-02,  3.96192622e-03,  2.84768023e-03,\n",
       "        2.53471032e-02, -3.29723942e-03,  3.56812813e-02,  2.86191360e-02,\n",
       "        1.19330973e-03,  1.35618273e-02, -2.06329389e-02,  3.84941777e-02,\n",
       "       -1.28232473e-02,  2.09548505e-02,  2.82098946e-02, -2.42918329e-02,\n",
       "       -2.15991832e-02, -7.97892623e-03, -5.00735200e-02,  5.44110953e-02])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 100)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([204])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "cos(torch.tensor(vecs[0]).view(1,-1), torch.tensor(vecs)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 생성된 벡터 표현을 저장해두자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname = 'data/sentence-embeddings/lsa-tfidf/lsa-tfidf.vecs'\n",
    "with open(output_fname, 'w') as f:\n",
    "    for doc_idx, vec in enumerate(vecs):\n",
    "        str_vec = [str(el) for el in vec]\n",
    "        f.writelines(titles[doc_idx] + \"\\u241E\" + raw_corpus[doc_idx] + '\\u241E' + ' '.join(str_vec) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 문서간의 유사도를 산출해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_docs(vec_mat, titles, index, k=10):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    \n",
    "    vec_mat = torch.tensor(vec_mat)\n",
    "    doc = vec_mat[index].view(1,-1)\n",
    "    cos_mat = cos(doc, vec_mat)\n",
    "    sim, indices = torch.topk(cos_mat,k+1)\n",
    "    \n",
    "    id_titles = []\n",
    "    for i in indices:\n",
    "        if i != index:\n",
    "            id_titles.append(titles[i])\n",
    "    return pd.Series(id_titles, np.array(sim[1:].detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.751197        loss\n",
       "0.731756         MLE\n",
       "0.686223         CRF\n",
       "0.620925     unsugen\n",
       "0.591298    logistic\n",
       "0.565547    gradient\n",
       "0.504974         VAE\n",
       "0.496815     softmax\n",
       "0.483263    NNtricks\n",
       "0.479947        MEMs\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_docs(vecs, titles, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 책에 쓰인 방법대로 유사문서 검색 및 시각화를 해보자.\n",
    "\n",
    "먼저 다음과 같이 모델을 불러오려고 했으나, tf.contrib의 nccl을 찾을 수 없다고 오류가 떠서 `model/sent_eval.py` 의 `from tune_utils import make_elmo_graph, make_bert_graph` 부분을 주석 처리함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maxparam',\n",
       " [('loss', 0.7511969326756819),\n",
       "  ('MLE', 0.7317555129892188),\n",
       "  ('CRF', 0.6862234059889061),\n",
       "  ('unsugen', 0.6209250503586896),\n",
       "  ('logistic', 0.5912982144408041),\n",
       "  ('gradient', 0.5655466890513458),\n",
       "  ('VAE', 0.5049736723248607),\n",
       "  ('softmax', 0.4968154562639956),\n",
       "  ('NNtricks', 0.48326250454696795),\n",
       "  ('MEMs', 0.47994716262040377)]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.sent_eval import LSAEvaluator\n",
    "model = LSAEvaluator('data/sentence-embeddings/lsa-tfidf/lsa-tfidf.vecs')\n",
    "model.most_similar(doc_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save @ between-sentences.png\n"
     ]
    }
   ],
   "source": [
    "model.visualize('between')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save @ sentences.png\n"
     ]
    }
   ],
   "source": [
    "model.visualize('tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='between-sentences.png'>\n",
    "\n",
    "<img src='sentences.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
